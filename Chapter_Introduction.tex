% !TEX root = More_PhD_Thesis.tex
\cleardoublepage
\chapter{Introduction}

	\section{Overview of Nuclear Physics}

	Nuclear physics deals with studying the properties of atomic nuclei.
	The nuclear
	landscape shown in Fig.~\ref{fig:Nuclear_landscape} has been the
	traditional playground for nuclear physics.  The questions historically
	driving nuclear physics have been: how do protons and neutrons
	make stable nuclei and rare isotopes?  What are the limits of nuclear
	existence (also referred to as drip lines)?
	What are the nuclear binding energies, excitation spectra, radii and so on?
	We would also like to describe nuclear reactions, make predictions about
	the shape of the nuclei and understand how the shape dictates the nuclear
	properties.
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.9\textwidth]%
	 {Introduction/Nuclear_landscape}
	 \caption{Nuclear landscape.  A total of $288$ isotopes (black squares) are
	  	stable on the time scale of the solar system.  As more protons
			or neutrons are added to these stable nuclei, we enter the regime of
			short-lived radioactive nuclei (green squares).  `Drip lines' mark the
			limit of nuclear existence, where the last nucleon is no longer bound.
			The uncertainties around drip lines (in red)
	  	were obtained by averaging the results of different theoretical models.
			Figure from \cite{Long_range_plan}.}
	 \label{fig:Nuclear_landscape}
	\end{figure}

	By the mid-1970s, it was generally accepted that the nucleons
	(proton and neutrons)
	and other hadrons are composed of quarks, and that the quarks are held
	together through the exchange of gluons.  The following decades witnessed
	rapid
	development in the theory of strong interactions, the fundamental force
	describing the interactions between quarks and gluons.  This theory goes
	by the name of Quantum Chromodynamics (QCD).  One of the active areas of
	investigation is obtaining the hadron structure from QCD.  This includes,
	for example, understanding the origin of proton spin, which is studied
	experimentally in experiments at Jefferson Laboratory.  A related focus area
	is understanding the nature of the quark-gluon plasma (QGP)---the phase
	in which the universe is believed to exist for up to a few milliseconds after
	the Big Bang.  The energies involved in this subfield are higher (few GeVs)
	than the energies in `traditional' nuclear physics (few MeVs) introduced in
	the opening paragraph.  Therefore it is conventional to refer to the two
	subfields as high-energy nuclear physics and low-energy nuclear physics.
	The work in this thesis will mainly focus on questions in low-energy nuclear
	physics (LENP).

	Apart from the questions at the core of LENP, mentioned in connection to
	Fig.~\ref{fig:Nuclear_landscape}, inputs from LENP are immensely important
	in other areas as well.  One such broad area is that of nuclear astrophysics.
	The majority of the stable
	and known nuclei shown in Fig.~\ref{fig:Nuclear_landscape} were formed in
	big bang, stellar, or supernova nucleosynthesis.  Inputs from LENP are
	critical in understanding the processes involved in nucleosynthesis and
	predicting the observed abundances of isotopes.
	Neutron stars are another fascinating astrophysical objects for low-energy
	nuclear physicists.  We would like to determine the equation of state for
	neutron star and understand how and why the stars explode.

	Finally, there are questions about the fundamental symmetries of the universe
	where
	nuclear physics hopes to make significant contributions.  For instance,
	why is there more matter than antimatter in the universe?  What is the nature
	of dark matter?  What is the nature of the neutrinos (Majorana or Dirac
	fermions) and how have they shaped the evolution of the universe?
	In fact, as we will see later, accurate calculations of nuclear matrix
	elements are critical for the experiments undertaken to understand the
	nature of the	neutrinos.

	In addition to the broad scientific impact of nuclear physics that we have
	already mentioned, it also has many real-life applications.  Our knowledge of
	nuclei and ability to produce them has led to an increase in the quality of
	life
	for the humankind.  Applications of nuclear physics encompass a diverse
	domain including but not limited to energy, security, medicine,
	radioisotope dating, and material sciences.


	\section{Checkered past; promising future}

	In 1935, Hideki Yukawa proposed the seminal idea of nuclear interaction
	being mediated by a massive boson \cite{Yukawa:1935xg}.   This could explain
	how protons and neutrons would stay bound in a nucleus overcoming the Coulomb
	repulsion between protons.  The fact that such a model described scattering
	data well at low energies (few MeVs) and the eventual discovery of pions in
	1947 led to a wide acceptance of this model.  Very soon other heavy mesons
	($\rho$, $\omega$, $\sigma$) were discovered as well.  Scattering experiments
	also indicated that the strength on the nuclear potential depended on
	distance and at short distances the potential was repulsive.

	By 1950, there emerged an industry for coming up with better nuclear
	potentials.  These boson-exchange models shared some common features.
	The long-range part of the nucleonic interaction was given by pion exchange,
	the intermediate range was governed by multiple (mostly two) pion exchange,
	and short-range repulsion was thought to be because of overlap of nucleons.
	When heavy mesons were discovered, they were added to the intermediate range
	sector.  (Pion being the lightest meson has the longest range.)
	These general considerations form the basis for phenomenological potentials
	used even today as seen in Fig.~\ref{fig:Nuclear_potentials}.
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.6\textwidth]%
	 {Introduction/nuclear_potentials}
	 \caption{AV18 \cite{Wiringa:1994wb}, Reid93 \cite{Stoks:1994wp}, and
	  	Bonn \cite{Machleidt:1989tm} potentials for $^1S_0$ channel as
			functions of internucleonic distance.  These potentials accurately
			describe neutron-proton scattering up to laboratory energies of
			300 MeV.  Regions I, II, and III correspond to long-range,
			intermediate-range, and short-range parts discussed in the text.
			Figure from \cite{Aoki:2008hh}. }
	 \label{fig:Nuclear_potentials}
	\end{figure}

	This intense effort is well summarized by Hans Bethe's quote in his essay
	`What Holds the Nucleus Together?' in Scientific American (1953):
	``In past quarter century physicists have devoted a huge amount of
	experimentation and mental labor to this problem -- probably more man-hours
	than have been given to any other scientific question in the history of
	mankind.''
	The boson models did not have a smooth sailing though.  In particular
	the intermediate range multi-pion sector was beset with problems.  The
	pessimism this resulted in is palpable in Marvin Goldberger's comment
	in 1960: ``There are few problems in nuclear theoretical physics which
	have attracted more attention that that of trying to determine the
	fundamental interaction between two nucleons.  It is also true that
	scarcely ever has the world of physics owed so little to so many...It
	is hard to believe that many of the authors are talking about the same
	problem or, in fact, that they know what the problem is.'' A running joke
	was that nuclear physics is really `unclear' physics!

	There was relatively slow progress with regards to the development of
	inter-nucleonic potential in 1970's and 80's.  However, this period saw
	a rapid development of QCD.  It was realized that the nucleons and pions
	are composed of quarks which are held together by exchange of gluons
	(cf.~Fig.~\ref{fig:nucleon_interaction}).
	This pushed the effort to derive the nuclear
	potential from the `fundamental' theory of QCD.

	%
	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[c]{0.43\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/nucleon_pion_exchange}
			\caption{Inter-nucleon interaction in 1940}
			\label{fig:nucleon_pion_exchange}
		\end{subfigure}
		\hspace{0.07\textwidth}
		\begin{subfigure}[c]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/nucleon_gluon_exchange}
			\caption{Inter-nucleon interaction in 1980}
			\label{fig:nucleon_gluon_exchange}
		\end{subfigure}
		\caption{Evolution of the inter-nucleon interaction picture over time.}
		\label{fig:nucleon_interaction}
	\end{figure}
	%
	However, the effort to replace the hadronic descriptions at ordinary nuclear
	densities with quark description as in Fig.~\ref{fig:nucleon_gluon_exchange}
	was not very fruitful.
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.6\textwidth]%
	 {Introduction/alpha_QCD_running}
	 \caption{Summary of measurements of the QCD coupling $\alpha_s$ as a
	 function of energy scale $Q$. }
	 \label{fig:alpha_QCD_running}
	\end{figure}
	%
	As seen in Fig.~\ref{fig:alpha_QCD_running}, the strength of the QCD
	coupling $\alpha_s$ increases with decreasing energies.  This makes
	QCD non-perturbative in the low-energy regime of nuclear physics limiting
	the success of analytical calculations.

	Another aspect that makes low-energy nuclear physics difficult is that it is
	a many-body problem.  It exhibits some emergent phenomena that are difficult
	to capture in reductionist approach.  This issue has been well-summarized by
	the famous article `More is different' by Phillip Anderson
	\cite{Anderson:1972pca} (albeit with a focus on many-body problem in
	condensed matter).

	Despite all these challenges, great strides have been made in LENP
	in the last few decades.  As indicated in Fig.~\ref{fig:Moores_law_LENP}
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.7\textwidth]%
	 {Introduction/Moores_law_LENP}
	 \caption{LENP version of Moore's law and its violation.  $Y$ axis is the mass
	  	number of nuclei that can be calculated from ab-initio calculations.
			In past few years, it has been possible to push the ab-initio frontier to
			heavier nuclei.  Figure courtesy of Gaute Hagen. }
	 \label{fig:Moores_law_LENP}
	\end{figure}
	%
	particularly the last few years have seen an explosion in capabilities of
	low-energy nuclear theory.  This phenomenal progress has been possible due
	to combination of a few factors --- new insights about the nuclear force,
	developments in the many-body technology, and the surge in computational
	capabilities.  In the following sections, we look briefly at each of these
	developments which will lead us to how author's PhD work fits into the
	bigger picture.


	\section{Understanding the Force}
	\label{sec:recent_advances}

	We saw that non-pertubativeness of QCD at low energies
	(cf.~Fig.~\ref{fig:alpha_QCD_running}) motivated phenomenological
	descriptions of nuclear forces.  The two popular categories of
	phenomenological interactions are the meson exchange models (which we
	touched upon during discussion of Fig.~\ref{fig:Nuclear_potentials}) and
	local phenomenological potentials \footnote{The potential is local if
	$V(r, r^\prime) = V(r) \delta(r - r^\prime)$}.
	Examples of these include potentials
	from Bonn interactions \cite{Machleidt:1989tm, Machleidt:1987hj,
	Machleidt:2000ge} and the Argonne interactions \cite{Wiringa:1994wb}.

	The Argonne potential is probably the most widely used phenemenological
	potential.  One of the reasons being that until recently, it was the only
	precision interaction usable for Quantum Monte Carlo calculations.  The
	Argonne interactions are built by writing down all the operators that
	satisfy the required symmetries---translational and Galilean invariance,
	rotational invariance in space and spin, rotational invariance in isospin,
	time reversal, and spatial reflection.  These operators are given below.
	\beq
	\wh{O}_i \in \{ \mathds{1}, \bm{\sigma_1} \cdot \bm{\sigma_2},
	S_{12}, \bm{L} \cdot \bm{S}, \bm{L}^2, \bm{L}^2 \bm{\sigma_1} \cdot
	\bm{\sigma_2}, (\bm{L} \cdot \bm{S})^2 \} \otimes
	\{\mathds{1}, \bm{\tau_1} \cdot \bm{\tau_2}\}
	\label{eq:AV18_14_operators}
	\eeq
	$S_{12} = 3 (\bm{\sigma_1} \cdot \wh{\bm{r}})
	(\bm{\sigma_2} \cdot \wh{\bm{r}}) - \bm{\sigma_1} \cdot \bm{\sigma_2}$
	is the tensor force.

	There are total of 14 operators in the Eq.~\eqref{eq:AV18_14_operators}.
	The AV18 potential has four more operators which are the charge-dependent
	and charge-symmetry breaking terms; they are small but needed to get
	$\chi^2/{\rm{dof}} \approx 1$ for np, nn, and pp scattering.  There are
	also AV8, AV6 potentials which use a limited set of operators.

	The AV18 potential is written as
	\beq
	\wh{V}_{18}(r) = \sum_{i = 1}^{18} V_i(r) \, \wh{O}_i \;.
	\eeq
	$r$ is the inter-nucleon separation and $V_i (r) = V_{{\rm{EM}}} + V_{\pi}
	+ V_{{\rm{short~range}}}$.  The coefficients in the potential are fit to
	nucleon scattering up to $350$ MeV, and to deuteron bound state properties
	such as binding energy, radii, and quadrupole moment.
	A similar exercise has been done for 3N (3-body) interactions.  However,
	the large number of possible three-body operators makes it difficult to
	get rid of model dependence.

	Meson exchange models are formulated in terms of exchange of mesons taking
	into account to the nature of mesons (scalar, vector, pseudo-scalar, so on).
	The masses are those of real mesons, but couplings are fit parameters.
	In the simplest form, the interaction is a sum of Yukawa potentials,
	\beq
	V = \left(\frac{-g_s^2}{4 \pi}\right) \frac{\ee^{-m_s r}}{r} +
	 		\gamma_1^\mu \gamma_{2 \mu} \left(\frac{-g_\omega^2}{4 \pi}\right)
			\frac{\ee^{-m_\omega r}}{r} +
			\gamma_1^5 \gamma_{2}^5 \, \bm{\tau_1} \cdot \bm{\tau_2}
			\left(\frac{-g_{\pi}^2}{4 \pi}\right)
			\frac{\ee^{-m_{\pi} r}}{r}\;.
	\eeq

	Meson exchange potentials and the AV18 potential share the common shortcoming
	that there is no scope for systematic	improvements.  It is also unclear
	how to seek model independence and do robust uncertainty quantification.
	These models also offer limited guidance on the strength and relevance of
	three- and higher-body forces.


	\subsection{Chiral EFT}

	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.6\textwidth]%
	 {Introduction/degrees_of_freedom2}
	 \caption{Hierarchy of degrees of freedom and associated energy scales in
	  	nuclear physics \cite{LRP:2007}.}
	 \label{fig:degrees_of_freedom}
	\end{figure}
	%
	An intriguing aspect of the world we live in, is that there are interesting
	phenomena at virtually all energy and length scale we can probe.  From TeV
	energies at the Large Hadron Collider (LHC) to the life-defining process of
	respiration which has the energy scale of only few meV, there are physical
	processes of interest at each step.  Nuclear physics spans a wide range of
	energy and length scales (cf.~Fig.~\ref{fig:degrees_of_freedom}); probably
	a wider range compared to most subfields.  This hierarchy provides both
	challenges and opportunities.

	Figure~\ref{fig:degrees_of_freedom} indicates the relevant degrees of freedom
	for the given energy scales.  Even though degrees of freedom are a matter of
	choice, in practice, appropriate degrees of freedom often dictate the
	success of a theory.  To quote Steven Weinberg \cite{Guth:1984rq}:
	``You can use any degrees of freedom you want, but if you use the wrong ones,
	you'll be sorry.''
	Weinberg in his seminal paper \cite{Weinberg:1978kz} applied the concept
	of effective field theory (EFT) to low-energy QCD.  This effort
	proceeds by writing down the most general Lagrangian consistent with the
	(approximate) symmetries.

	QCD has the expected symmetries of translational, Galilean, and rotational
	invariance, and spatial reflection and time reversal.  Along with that,
	in the limit of vanishing quark masses, QCD Lagrangian also possess an exact
	chiral symmetry \cite{Peskin1995a}.  If the chiral symmetry holds,
	``left''- and
	``right''-handed fields do not mix.  Like with any other continuous symmetry,
	spontaneous breaking of chiral symmetry leads to massless Goldstone boson(s).
	The masses of the up and down quark (quarks relevant in LENP) are both small
	($\sim 2 - 6$ MeV \cite{Agashe:2014kda}), but non-zero.  Therefore the
	chiral symmetry of QCD Lagrangian is only approximate and the resulting
	Goldstone boson --- pion --- is light (compared to mass of nucleon), but not
	massless.

	Chiral EFT ($\chi$-EFT) uses nucleons and pions as degrees of freedom.
	Heavy mesons are
	integrated out.  The crucial difference that distinguishes $\chi$-EFTs from
	meson theories of 1950s is that they are constrained by the chiral symmetry.
	Broken chiral symmetry serves as a connection with the underlying theory of
	QCD.
	A major advantage of $\chi$-EFT is that it permits systematic improvements
	and allows the possibility of having reliable uncertainty quantification.


	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.8\textwidth]%
	 {Introduction/ChEFTTerms}
	 \caption{Diagrams in a chiral Lagrangian at each order.  Solid lines are the
	 nucleons and dashes lines pions.  Figure from \cite{Machleidt:2011zz}.}
	 \label{fig:ch_EFT_diagrams}
	\end{figure}
	%
	Diagrams in a $\chi$-EFT are shown in Fig.~\ref{fig:ch_EFT_diagrams}.  The
	three- and higher-body forces appear naturally in $\chi$-EFT.  The coupling
	constants at the vertices in Fig.~\ref{fig:ch_EFT_diagrams} are called
	low-energy coupling constants (LECs); they encode the QCD physics and
	cannot be calculated in $\chi$-EFT.  LECs are fitted to experimental data.
	Lattice QCD provides a promising method for extracting them in near
	future (cf.~Subsec.~\ref{subsec:lattice_theories}).  Despite some concerns
	over
	power-counting (\emph{cite Bira here? and Delta incorporation work}), it is
	fair to say that $\chi$-EFT has been a major breakthrough in low-energy
	nuclear theory.  For more details and applications of $\chi$-EFT, please
	see Refs.~\cite{Machleidt:2011zz} and \cite{Epelbaum:2005pn}.


	\section{Many-body methods}

	% \begin{figure}[htbp]
	%  \centering
	%  \includegraphics[width=0.65\textwidth]%
	%  {Introduction/nuclear_landscape_with_methods}
	%  \caption{Domain of various methods used in nuclear physics.  Figure
	%  	from \cite{LRP:2007}.}
	%  \label{fig:nuclear_landscape_with_methods}
	% \end{figure}

	We learned in kindergarten quantum mechanics that a two-body problem
	can be solved by reducing it to one-body problem.  The many-body
	problem is far more relevant to the nuclear physics, and is more
	challenging.
	For the nuclear properties such as the bound state energies, the
	equation we need to solve is the time-independent Schr\"{o}dinger equation
	\beq
	\wh{H} \ket{\Psi} = E \ket{\Psi} \;.
	\label{eq:Schrodinger_many_body}
	\eeq
	%
	The Hamiltonian $\wh{H}$ is given by
	\beq
	\wh{H} = \sum_{\rm particles} \wh{T} + \sum_{\rm pairs}
	\wh{V}^{(2)}_{\rm pair} + \sum_{\rm triplets}
	\wh{V}^{(3)}_{\rm triplets} + \cdots \;,
	\eeq
	where $\wh{T}$ is the kinetic energy and $\wh{V}^{(2)}, \, \wh{V}^{(3)}$ are
	the nuclear potentials (e.g., from $\chi$-EFT or phenomenological potentials).

	Over years various methods have been developed to tackle
  this problem.  We we will list some of the broad categories below.
	We only provide
  a brief explanation for each of them and refer the reader to relevant
  references for details.
	\bi
	\li
	\emph{Direct diagonalization:}
	This category involves expanding the many-body wave function $\ket{\Psi}$ in
	an appropriate complete basis.  Very often this basis is chosen to be the
	slater determinant of Harmonic oscillator (HO) wave functions.
	HO has all the symmetries of nuclear many-body problem.  HO also allows
	separation of center of mass motion from relative motion, ensuring that only
	intrinsic properties are being calculated.

	Finite computational power forces us to truncate the infinite sum of
	slater determinants at some point.  One of the common truncation scheme is
	the $\Nmax$ truncation.  It keeps all $A$ particle states
	$\ket{n_1 l_1 n_2 l_2 \ldots n_A l_A}$ such that
	\beq
	\sum_i 2 n_i + l_i \leq \Nmax \;.
	\eeq
	%
	With $\Nmax$ truncation, solving the many-body Schr\"{o}dinger equation
	(Eq.~\eqref{eq:Schrodinger_many_body}) becomes a matrix diagonalization
	problem, and can be solved using standard algorithms.

	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[c]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/He4_vs_nmax_highlight_before}
			\caption{$^4$He energies as a function of $\Nmax$.}
			\label{fig:He4_vs_Nmax}
		\end{subfigure}
		\hspace{0.07\textwidth}
		\begin{subfigure}[c]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/Li6_vs_nmax_highlight_before}
			\caption{$^6$Li energies as a function of $\Nmax$.}
			\label{fig:Li6_vs_Nmax}
		\end{subfigure}
		\caption{Convergence of energies as a function of the truncation
			parameter $\Nmax$ \cite{Jurgenson:2010wy}.}
		\label{fig:Nmax_convergence}
	\end{figure}
	%
	Figure~\ref{fig:Nmax_convergence} shows convergence plots for ground state
	energies for $^4$He and	$^6$Li.  As $\Nmax$ is increased the energies
	approach the asymptotic values \footnote{When the asymptotic value doesn't
	match the experimental value, it points to the shortcomings in nuclear
	Hamiltonian.}.
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.45\textwidth]%
	 {Introduction/ncsm_matrix_dimension_vs_Nmax2}
	 \caption{Matrix dimension grows factorially with the number of nucleons.
	 	Figure courtesy of Pieter Maris.}
	 \label{fig:matrix_dimension_growth}
	\end{figure}
	%
	A limitation of diagonalization method is that the size of the Hamiltonian
	matrix that we need to diagonalize grows rapidly as we go to higher $A$ and
	higher $\Nmax$ (cf.~Fig.~\ref{fig:matrix_dimension_growth}).  This often
	forces us to truncate the basis before the convergence is reached
	(cf.~Fig.~\ref{fig:Li6_vs_Nmax}), and necessitates development of
	reliable extrapolation schemes.  Chapter~\ref{chap:Extrapolation}
	describes author's original work devoted to the development of
	extrapolation schemes relevant to this problem.

	Many different methods fall under the broad umbrella of diagonalization
	methods.  This includes the traditional shell model~\cite{Brown:2001zz},
	the No Core Shell Model (NCSM)~\cite{Barrett:2013nh},
	the No Core Full Configuration (NCFC)~\cite{Maris:2008ax}, and the Importance
	Truncated No Core Shell Model (IT-NCSM)~\cite{Roth:2009cw}.
	The last three methods primarily differ in the nature of truncation
	they employ.

	\li
	\emph{Monte Carlo methods:}
	These methods use the imaginary time evolution of the Schr\"{o}dinger
	equation
	\beq
	-\partial_\tau \ket{\Psi(\tau)} = \wh{H} \ket{\Psi(\tau)} \;.
	\label{eq:imag_time_SE}
	\eeq
	This equation can be solved by making an ansatz for
	$\ket{\Psi(\tau = 0)} \equiv \ket{\Psi_{\rm trial}}$.
	$\ket{\Psi_{\rm trial}}$ can be expanded in complete basis of eigenvectors
	of $\wh{H}$ as
	\beq
	\ket{\Psi_{\rm trial}} = c_0 \ket{\Psi_0} +
	\sum_{i \neq 0} c_i \ket{\Psi_i} \;.
	\label{eq:Psi_trial_expansion}
	\eeq
	As long as the trial wave function is not orthogonal to the actual ground
	state, i.e, $c_0 \neq 0$ in Eq.~\eqref{eq:Psi_trial_expansion}, it can
	be shown that the imaginary time evolution (Eq.~\eqref{eq:imag_time_SE})
	projects out the ground state in $\tau \to \infty$ limit.  There are many
	different ways to do the stochastic time evolution of
	Eq.~\eqref{eq:imag_time_SE} to project out the ground state.  The two
	most popular in nuclear physics are the Green's function Monte Carlo (GFMC)
	\cite{Pieper:2002ne, Pieper:2007ax}
	and the Auxiliary Field Diffusion Monte Carlo (AFDMC)
	\cite{Gandolfi:2007hs, Gezerlis:2014zia}.  Monte Carlo methods require that
	the nuclear potential used be local.  AV18 potential has been the potential
	of choice for these methods.  Recently, it has been possible to
	derive the low-order $\chi$-EFT in local form, making its use possible
	for Monte Carlo methods \cite{Gezerlis:2014zia}.

	\li
	\emph{Coupled Cluster:} The energy of the many-body state $\Psi$ is given
	by
	\beq
	E = \mbraket{\Psi}{\wh{H}}{\Psi}\;.
	\label{eq:Schrodinger_CC}
	\eeq
	%
	The coupled cluster (CC) method tries to built the state $\Psi$ from the
	reference state $\Phi$ (which for instance can be the Hartree-Fock state)
	using the transformation
	\beq
	\ket{\Psi} = \ee^T \ket{\Phi}\;.
	\label{eq:CC_defining_equation}
	\eeq
	%
	Thus, Eq.~\eqref{eq:Schrodinger_CC} becomes
	\beq
	E = \mbraket{\Phi}{\ee^{-T}\wh{H} \ee^{T}}{\Phi}\;.
	\eeq
	%
	The cluster operator $T$ in Eq.~\eqref{eq:CC_defining_equation}
	is defined with respect to the reference state.
	\beq
	T = T_1 + T_2 + \ldots + T_A \;.
	\label{eq:T_expansion}
	\eeq
	%
	$T_n$ generates $n$-particles-$n$-holes excitations.  In practice,
	Eq.~\eqref{eq:T_expansion} is truncated at $T_2$.  CC (truncated at $T_2$)
	scales much better than diagonalization or Monte Carlo methods discussed
	above and is therefore possible to use it for medium-mass nuclei
	\cite{Hagen:2013nca}.

	\li
	\emph{Density Functional Theory:} Density Functional Theory (DFT) is based
	on the principle that the many-body ground state $\Psi(\bm{r_1}, \bm{r_2},
	\ldots, \bm{r_A})$ can be written as a functional of the density $\rho$,
	i.e, $\Psi = \Psi[\rho]$.
	Consequently, the energy (or any other observable) can be written
	as a functional of density
	\beq
	E[\rho] = \mbraket{\Psi[\rho]}{\wh{H}}{\Psi[\rho]} \;.
	\eeq

	DFT proceeds by writing down a energy density functional (EDF) guided by
	intuition and general theoretical arguments
	\cite{Drut:2009ce, Dobaczewski:2010gr}.  Inputs from experiments
	and exact calculations for simple few-body systems are also used for
	constructing the EDF.  The properties of the physical system are then found
	by the two step minimization of the EDF---first minimization is at a fixed
	density $\rho(\bm{r})$ and the second minimization is over $\rho(\bm{r})$.
	Once the EDF is decided upon, DFT does not scale prohibitively with
	the number of nucleons $A$, and is therefore the most popular for
	calculating properties of heavy-mass nuclei.

	\li
	\emph{IM-SRG:} In-Medium Similarity Renormalization Group (IM-SRG) is
	based on the SRG technique we will look at in Subsec.~\ref{subsec:SRG_intro}.
	It uses a series of continuous unitary transformations to decouple
	the ground state of the many-body Hamiltonian from the excitations.
	IM-SRG has made it possible to apply the ab-initio (starting with 2N and
	3N forces) methods to medium-mass nuclei and beyond.  Please see the
	Ref.~\cite{Hergert:2015awm} for a recent review on progress acheived by
	IM-SRG.
	\ei

	\subsection[``It from the bit"---lattice theories]%
	{``It from the bit"\footnote{The phrase ``It from the bit'' was originally
	used by John Wheeler while elucidating his ideas on digital physics}
	---lattice theories}
	\label{subsec:lattice_theories}

	\medskip
	\subsubsection{Lattice QCD}

	We saw through Fig.~\ref{fig:alpha_QCD_running} that the largeness of the
	QCD coupling at low-energies makes it unamenable to analytical calculations.
	A well-established non-perturbative approach in this regime is lattice
	QCD \cite{Savage:2015eya}.  In lattice QCD, one discretizes space-time;
	fields representing quarks are defined at lattice sites and
	the gluon fields are defined on links connecting neighboring sites.

	Ideally we would like the lattice size to be as large as possible and the
	lattice spacing to be as
	small as possible.  However, lattice calculations are computationally
	extremely intensive, thereby severely
	constraining the lattice size and spacing.  Moreover, the computational
	cost of simulations
	scales with the quark mass roughly as $m_q ^{-4}$
	\cite{Detmold:2003rq}.  The simulations
	are therefore often done at quark masses larger than the physical quark
	masses.  From Gell-Mann-Oakes-Renner relation $m_\pi ^2 \sim m_q$,
	and therefore the pion mass
	in the lattice calculations is larger than its physical value as well.
	The results
	are then extrapolated down to physical quark (or pion) mass.
	In the nuclear case, for accurate extrapolation one must also take into
	account the crucial
	non-analytic structure associated with chiral symmetry breaking
	\cite{Detmold:2001jb}.

	Recent calculations have been able to use the physical pion
	mass though only for very light systems \cite{Durr:2010aw}.  In near future,
	we hope to extract the low-energy constants in Fig.~\ref{fig:ch_EFT_diagrams}
	from lattice QCD.



	\medskip
	\subsubsection{Lattice EFT}

	Quarks and gluons have many degrees of freedom in terms of spin, color
	charge, and flavor, which makes getting nuclear physics from lattice QCD
	computationally difficult.  An alternative approach is to have nucleons
	on the lattice site rather than the quarks.
	%
	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[c]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/lattice_QCD}
			\caption{Lattice QCD~~~~~~~~~~~~}
			\label{fig:lattice_QCD}
		\end{subfigure}
		\hspace{0.1\textwidth}
		\begin{subfigure}[c]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/lattice_EFT}
			\caption{Lattice EFT~~~~~~~~~~~~}
			\label{fig:lattice_EFT}
		\end{subfigure}
		\caption{Lattice QCD vs.\ lattice EFT.  Figure courtesy of Dean Lee.}
		\label{fig:lattice_QCD_EFT}
	\end{figure}
	%
	The difference between the two methods is illustrated in
	Fig.~\ref{fig:lattice_QCD_EFT}.  Lattice EFT combines the framework of
	effective field theory and computational lattice methods and is a
	promising tool for studying light nuclear systems.

	Lattice theories inherently come with the associated graininess and the
	finite size.  Thus, we have a cutoff for both the maximum length and the
	maximum momentum scale that we can have on a lattice.  This is qualitatively
	similar	to the harmonic oscillator basis truncation problem that we will
	look extensively at in Chapter~\ref{chap:Extrapolation}.  Various methods
	have been used to obtain the continuum limit from the lattice.  We will touch
	upon these in
	Subsec.~\ref{subsec:IR_front}, where we look at similarities and differences
	between the lattice methods and our work with oscillator basis truncation.


	\section{RG techniques}

	We saw in Fig.~\ref{fig:Nmax_convergence} that the convergence in many-body
	calculations is slow.  To understand why this is the case, recall from
	Fig.~\ref{fig:Nuclear_potentials} that the nuclear potentials have a strong
	short-range repulsion.  This hard core leads to high-momentum components in
	the potentials.
	%
	\begin{figure}[htbp]
	 \centering
	 \includegraphics[width=0.45\textwidth]%
	 {Introduction/vnn_1S0_kvnn06_k_cntr}
	 \caption{The $^1 S_0$ AV18 potential in momentum space.  Figure from
	 \cite{Furnstahl:2013oba}.}
	 \label{fig:momentum_space_AV18}
	\end{figure}
	%
	This can be seen in Fig.~\ref{fig:momentum_space_AV18} where we plot the
	AV18 potential in the $^1 S_0$ channel in momentum space.  We note that
	for the AV18 potential, $V(k, k^{\prime}) \to 0$ only for $k, \, k^\prime
	\gtrsim 25 {\rm{~fm}}^{-1}$.  However, the Fermi momentum of the nucleon
	in heavy nucleus like $^{208}$Pb is only about $1.2 {\rm{~fm^{-1}}}$.
	The Fermi momentum sets the momentum scale of low-energy nuclear
	processes we wish to study.  Thus, we have a mismatch of resolution scales;
	the processes we wish to describe are $1 - 2 {\rm{~fm^{-1}}}$
	($200 - 400 {\rm{~MeV}}$), whereas the momentum scale in the potential
	is much higher.   To use the analogy due to Tom Banks, `it is like trying to
	understand the properties of waves in the ocean in terms of Feynmann
	diagrams'.  Though in principle this can be done, it potentially makes
	calculations intractably complicated.

	This bring us back to Fig.~\ref{fig:degrees_of_freedom}.  The progression
	from top to bottom in Fig.~\ref{fig:degrees_of_freedom} can be thought of as
	reduction in resolution.  This can be established theoretically using the
	renormalization group (RG) methods.  As mentioned before, the focus in LENP
	is in the intermediate region, where nucleons are the degrees of freedom.
	But even within this limited region, the concept of changing resolution
	by RG methods has been extremely advantageous \cite{Furnstahl:2013oba}.

	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[c]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/nucleus_high_resolution}
			\caption{Nucleus under a high-resolution probe.}
			\label{fig:nucleus_high_resolution}
		\end{subfigure}
		\vskip 0.6cm
		\begin{subfigure}[c]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/nucleus_low_resolution}
			\caption{Fine details (nucleon substructure) not resolved when
			probed at low energies.}
			\label{fig:nucleus_low_resolution}
		\end{subfigure}
		\vskip 0.6cm
		\begin{subfigure}[c]{0.7\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/1200px-Georges_Seurat_-_A_Sunday_on_La_Grande_Jatte_--_1884_-_Google_Art_Project}
			\caption{A painting by Georges Seurat, which is an example of
			pointillism.  Small, distinct dots of color are applied to form a
			pattern.  The pattern would be lost under a high-resolution probe.
			Image from Wikipedia.}
			\label{fig:Georges_Seurat}
		\end{subfigure}
		%
		\caption{Physics interpretation often changes with resolution.}
		\label{fig:physics_interpretation_resolution}
	\end{figure}
	%
	We would like to stress that contrary to the popular notion, high resolution
	is not always the best thing, especially when the processes we are looking at
	are low-momentum.  Also, though the value of the calculated observable is
	independent of the resolution, the physical interpretation often changes
	with the resolution.
	This is illustrated in Fig.~\ref{fig:physics_interpretation_resolution}.

	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/vsrg_cut_1S0_kvnn06_lam5_cut45_reg_0_3_0_ksq}
			\caption{An illustration of a low-pass filter.}
			\label{fig:low_pass_filter_example}
		\end{subfigure}
		\hspace{0.07 \textwidth}
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/srg_1S0_phases_bare_cut_v2}
			\caption{$^{1}S_0$ phase shifts for the AV18 potential and for the AV18
			potential after the low-pass filter which sets $V(k, k^\prime) = 0
			{\rm{~for~}} k, \, k^\prime > 2 {\rm{~fm^{-1}}}$.}
			\label{fig:low_pass_filter_effect}
		\end{subfigure}
		%
		\caption{Low-pass filter on nuclear potential---illustration and effect on
			phase shifts.}
		\label{fig:low_pass_filter_example_effect}
	\end{figure}
	%
	One of the methods to get rid of the high-momentum components is to apply
	a `low-pass filter' on the potential
	(cf.~Fig.~\ref{fig:low_pass_filter_example}).  This is routinely done, for
	example, in image processing.  Compression of a digital photograph is
	achieved by Fourier transforming it, setting the high-momentum modes in
	the Fourier transform equal to zero, and then transforming back.
	However, as seen in Fig.~\ref{fig:low_pass_filter_example}, the
	straightforward application of a low-pass filter fails to reproduce
	nuclear phase shifts even at low energies.

	This failure of low-pass filter can be understood by recalling from
	Fig.~\ref{fig:momentum_space_AV18} that the high and low-momentum
	modes are coupled.  For instance, consider (schematically) the expression
	for the phase shift in perturbation theory
	\beq
	\mbraket{k}{\wh{V}}{k} + \sum_{k^\prime}
	\dfrac{\mbraket{k}{\wh{V}}{k^\prime} \mbraket{k^\prime}{\wh{V}}{k}}
	{(k^2 - {k^\prime}^2)/m} + \cdots \;.
	\label{eq:phase_shift}
	\eeq
	The second term in Eq.~\eqref{eq:phase_shift} involves sum over off-diagonal
	matrix elements of $\wh{V}$.  Therefore even phase shifts for small $k$
	will have significant contributions from high $k^\prime$ if the coupling
	matrix elements $\mbraket{k}{\wh{V}}{k^\prime}$ are large.

	The correct way to get rid of the high-momentum components is to use the
	RG evolution and lower the momentum cutoff in small steps.
	%
	\begin{figure}[htbp]
		\centering
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/vlowk_schematic}
			\caption{The $V_{{\rm{low~}}k}$ running in $\Lambda$.}
			\label{fig:V_lowk_running}
		\end{subfigure}
		\hspace{0.1 \textwidth}
		\begin{subfigure}[t]{0.4\textwidth}
			\centering
			\includegraphics[width=\textwidth]
			{Introduction/srg_schematic}
			\caption{The SRG running in $\lambda$.}
			\label{fig:V_srg_running}
		\end{subfigure}
		%
		\caption{Schematic illustration of two types of RG evolution.  For given
		$\Lambda_i$ or $\lambda_i$ the matrix elements outside the corresponding
		lines are negligible.  This decouples the high-momentum modes from the
		low-momentum ones.  Figure from \cite{Furnstahl:2013oba}.}
		\label{fig:V_lowk_V_srg_running}
	\end{figure}
	%
	Two common choices for RG running are shown in
	Fig.~\ref{fig:V_lowk_V_srg_running}.  The RG running shown in
	Fig.~\ref{fig:V_lowk_running} is referred to as ``$V_{{\rm{low~}} k}$'' was
	historically developed first.  It attempts
	to get the potential in the low-pass-filter form through successive RG
	transformations \cite{Bogner:2009bt, Bogner:2003wn}.  Though successful
	for two-nucleon forces, it has been difficult to systematically treat
	many-body forces in the $V_{{\rm{low~}} k}$ framework.

 	A more recent approach through the similarity renormalization group (SRG) is
	illustrated in Fig.~\ref{fig:V_srg_running}.  SRG running drives the
	potential to a band-diagonal form.  It is possible to systematically account
	for the many-body forces in the SRG framework.  We will be using
	the SRG framework in our work presented in Chapter~\ref{chap:factorization}.
	Next we present a brief introduction to the SRG technique.


	\subsection{SRG}
	\label{subsec:SRG_intro}

	SRG equation.  continuous Unitary transformation.

	Effect of SRG.

	3N forces.  Factorization blob figure.

	Couple sentences about connecting to IM-SRG.



	\section{Path forward for LENP}

	UQ important.

	Add plot from P.\ Vogel showing differences in neutrinoless double beta decay
	calculations done using different methods.

	\section{Thesis organization}
